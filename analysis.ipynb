{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_characters = {\n",
    "    'sokka', 'aang', 'katara', 'zuko', 'toph', 'iroh', 'zuko', 'azula', # 1000+ non-stopwords / 3000+ total words\n",
    "    'zhao', 'jet', 'suki', 'hakoda', 'bumi',                            # 400+ non-stopwords / 1000+ total words\n",
    "}\n",
    "minor_characters = {\n",
    "    'hama', 'ozai', 'guru pathik', 'mai', 'roku', 'long feng',          # 350+ non-stopwords\n",
    "    'the mechanist', 'piandao', 'warden', 'bato', 'ty lee',\n",
    "    'jeong jeong', 'pakku', 'earth king', # 'zhang leader'\n",
    "    'professor zei', 'joo dee', 'chong', 'yue',                         # 200+ non-stopwords\n",
    "    'appa', 'momo'                                                      # :)\n",
    "}\n",
    "aliases = {\n",
    "    'fire lord ozai': 'ozai', 'ruko': 'roku', 'princess yue': 'yue', 'princess yue / moon spirit': 'yue', 'monk gyatso': 'gyatso',\n",
    "    'themechanist': 'the mechanist', 'zuko/blue spirit': 'zuko', 'king bumi': 'bumi', 'master pakku': 'pakku'\n",
    "}\n",
    "multiword_words = {\n",
    "    'jeong jeong': 'jeong_jeong', 'long feng': 'long_feng', 'ty lee': 'ty_lee', 'joo dee': 'joo_dee', 'wan shi tong': 'wan_shi_tong', \n",
    "    'twinkle toes': 'twinkletoes', 'twinkle-toes': 'twinkletoes'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "def get_ep_character_bows(script_lines):\n",
    "    \"\"\"Return a set of bags of words for this episode as a mapping from characters to words to counts.\"\"\"\n",
    "    bow = defaultdict(Counter)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        line = line.lower()\n",
    "        # Get all non-stop words\n",
    "        stop_words = set(w for w in stopwords.words('english') \n",
    "            + ['us', 'get', 'like', 'thats', 'go', 'going', 'cant', 'yeh', 'oh', 'got', 'hey', \n",
    "               'yeah', 'uh', 'whats', 'could', 'shall', 'gonna', 'okay', 'one'] \n",
    "        )\n",
    "        # Make a special case to treat multi-words as one word\n",
    "        words = [w.replace('_', ' ') for w in\n",
    "            re.findall(r'\\w+', \n",
    "                reduce(lambda acc, k: acc.replace(k, multiword_words[k]), multiword_words.keys(), line)\n",
    "            )]\n",
    "        bow[speaker]['total_count'] += len(words)\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        bow[speaker].update(words)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ep_total_word_count(script_lines):\n",
    "    return sum(len(re.findall(r'\\w+', line.split('::')[1])) for line in script_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_bows(bows):\n",
    "    \"\"\"Merge a collection of bags of words into one. Does not modify the input.\"\"\"\n",
    "    combined_bow = Counter()\n",
    "    for bow in bows:\n",
    "        combined_bow.update(bow)\n",
    "    return combined_bow\n",
    "\n",
    "def combine_character_bows(character_bows):\n",
    "    \"\"\"Merge a collection of mappings from characters to bags of words into one. Does not modify the input.\"\"\"\n",
    "    all_characters = set(k for bow in character_bows for k in bow.keys())\n",
    "    combined_bow_set = {c: combine_bows(bow_map[c] for bow_map in character_bows) for c in all_characters}\n",
    "    return defaultdict(Counter, combined_bow_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is a bad metric -- it just finds words that this character said once and nobody else ever said.\n",
    "def get_words_by_bayes(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, p), where w is a word \n",
    "    and p is the probability that a random occurrence of the word was spoken by that character.\n",
    "    The lists are sorted by descending probability.\n",
    "    \"\"\"\n",
    "    # Compute the total number of occurrences for each word\n",
    "    combined_bow = combine_bows(character_bows.values())\n",
    "    # p(character | word) = [# times character says word] / [# occurrences of word]\n",
    "    words_by_bayes = {\n",
    "        c: sorted(\n",
    "            [(w, character_bows[c][w] / combined_bow[w]) for w in combined_bow.keys()],\n",
    "            key=lambda wp: wp[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "# Since we have very few characters, we want to weight idf highly (with a lower base)\n",
    "base = 1.2\n",
    "\n",
    "# I experimented with these parameters but ultimately decided they were more harmful than helpful on such a small dataset\n",
    "def get_words_by_tfidf(character_bows, character_set=None, sublinear_tf=False, filter_df_1=False):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, s), where w is a word \n",
    "    and s is the tf-idf score of that word for that character. The lists are sorted \n",
    "    by descending score.\n",
    "    \"\"\"\n",
    "    # If no explicit character set is passed, just use all characters\n",
    "    character_set = character_set or character_bows.keys()\n",
    "    # Compute the number of characters that said each word\n",
    "    all_words = set(k for bow in character_bows.values() for k in bow.keys())\n",
    "    n_chars = len(character_set)\n",
    "    idf = {\n",
    "        w: 1 + log(n_chars / (1 + df), base)\n",
    "        for w in all_words if (df := sum(character_bows[c][w] > 0 for c in character_set)) > (1 if filter_df_1 else 0)\n",
    "    }\n",
    "    # tf-idf = tf(w, c) / (1 + log(N / (1 + df(w))))\n",
    "    words_by_tfidf = {\n",
    "        c: sorted([\n",
    "                (w, ((1 + log(v)) if sublinear_tf else v) * idf[w]) \n",
    "                for w, v in character_bows[c].items() if not filter_df_1 or w in idf\n",
    "            ],\n",
    "            key=lambda ws: ws[1],\n",
    "            reverse=True\n",
    "        ) for c in character_set\n",
    "    }\n",
    "    return words_by_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_character_sentence_sentiments(script_lines):\n",
    "    \"\"\"\n",
    "    Return a mapping from each character to a list of all their sentences along with the\n",
    "    computed sentiment scores (positive, negative, neutral) for each sentence.\n",
    "    \"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    char_sentence_sentiments = defaultdict(list)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        if speaker not in important_characters | minor_characters:\n",
    "            continue\n",
    "        \n",
    "        char_sentence_sentiments[speaker].extend(\n",
    "            (sid.polarity_scores(sentence), sentence) for sentence in tokenizer.tokenize(line)\n",
    "        )\n",
    "\n",
    "    return char_sentence_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_character_sentence_sentiments(target_map, added_map):\n",
    "    \"\"\"\n",
    "    Given two maps from characters to lists of sentence sentiments, combine all lists with the same key.\n",
    "    The first argument will be modified.\n",
    "    \"\"\"\n",
    "    for char in added_map.keys():\n",
    "        target_map[char].extend(added_map[char])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters_and_sentiment_frequencies(char_sentence_sentiments):\n",
    "    \"\"\"\n",
    "    Given a map from characters to lists of sentence sentiments, return a map\n",
    "    from characters to lists of the frequency of positive, negative, and neutral\n",
    "    sentiments.\n",
    "    \"\"\"\n",
    "    sentiment_freqs = defaultdict(lambda: defaultdict(int))\n",
    "    for char, sentiments in char_sentence_sentiments.items():\n",
    "        for (scores, _) in sentiments:\n",
    "            sentiment_freqs[char][\n",
    "                'pos' if scores['compound'] > 0.05 else 'neg' if scores['compound'] < -0.05 else 'neu'\n",
    "            ] += 1/len(sentiments)\n",
    "    return sentiment_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4.80178401692393\n7.379666948106317\n2.577882931182385\n6.379666948106316\n2.577882931182385\n4.80178401692393\n2.0\n3.0551242323191636\n"
    }
   ],
   "source": [
    "season_bows_collection = defaultdict(list)\n",
    "season_bows = {}\n",
    "season_tfidf = {}\n",
    "season_tfidf_minor = {}\n",
    "season_bayes = {}\n",
    "total_word_count = {}\n",
    "char_sentence_sentiments = defaultdict(list)\n",
    "for season in range(1, 4):\n",
    "    for episode in range(1, 21):\n",
    "        ep_num = f'{season}{str(episode).zfill(2)}'\n",
    "        with open(f'transcripts/{ep_num}.txt') as f:\n",
    "            script_lines = f.readlines()\n",
    "            season_bows_collection[season].append(get_ep_character_bows(script_lines))\n",
    "            merge_character_sentence_sentiments(char_sentence_sentiments, get_character_sentence_sentiments(script_lines))\n",
    "            total_word_count[ep_num] = get_ep_total_word_count(script_lines)\n",
    "    season_bows[season] = combine_character_bows(season_bows_collection[season])\n",
    "    # It helps to compute tf-idf scores for major characters only (excluding minor characters) or\n",
    "    # else depth-of-character words like \"know\", \"want\", \"think\" are weighted too highly\n",
    "    season_tfidf[season] = get_words_by_tfidf(season_bows[season], character_set=major_characters)\n",
    "    season_tfidf_minor[season] = get_words_by_tfidf(season_bows[season], character_set=major_characters | minor_characters)\n",
    "    season_bayes[season] = get_words_by_bayes(season_bows[season])\n",
    "season_bows['all'] = combine_character_bows([bows for s in range(1, 4) for bows in season_bows_collection[s]])\n",
    "season_tfidf['all'] = get_words_by_tfidf(season_bows['all'], character_set=major_characters)\n",
    "season_tfidf_minor['all'] = get_words_by_tfidf(season_bows['all'], character_set=major_characters | minor_characters)\n",
    "season_bayes['all'] = get_words_by_bayes(season_bows['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('aang', 319.08412104554384),\n ('gran', 240.89990494774008),\n ('sokka', 206.0),\n ('toph', 150.84638444002218),\n ('waterbending', 81.63032828770682),\n ('appa', 72.18072207310678),\n ('nini', 64.96481471753644),\n ('mum', 64.96481471753644),\n ('haru', 63.81962418616323),\n ('momo', 63.30073702403216),\n ('mom', 59.34444096003015),\n ('pole', 59.34444096003015),\n ('mother', 59.34444096003015),\n ('earthbending', 59.34444096003015)]"
     },
     "metadata": {},
     "execution_count": 281
    }
   ],
   "source": [
    "season_tfidf['all']['katara'][1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "17"
     },
     "metadata": {},
     "execution_count": 262
    }
   ],
   "source": [
    "season_bows['all']['zuko']['firebending']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('little', 0.5609805604940379),\n ('nation', 0.5609805604940379),\n ('great', 1.0),\n ('free', 1.477241301136777),\n ('better', 1.477241301136777),\n ('anything', 1.477241301136777),\n ('call', 2.0),\n ('girl', 2.0),\n ('everything', 2.0),\n ('mission', 2.577882931182385),\n ('sea', 2.577882931182385),\n ('taking', 2.577882931182385),\n ('comet', 2.577882931182385),\n ('food', 3.223901085741545),\n ('idea', 3.223901085741545),\n ('happy', 3.223901085741545),\n ('move', 3.223901085741545),\n ('kyoshi', 3.223901085741545),\n ('rock', 3.95629606400201),\n ('technique', 3.95629606400201),\n ('middle', 3.95629606400201),\n ('perfect', 3.95629606400201),\n ('iroh', 3.95629606400201),\n ('decided', 3.95629606400201),\n ('fast', 3.95629606400201),\n ('comes', 3.95629606400201),\n ('figure', 4.80178401692393),\n ('tried', 4.80178401692393),\n ('dark', 4.80178401692393),\n ('sight', 4.80178401692393),\n ('stick', 4.80178401692393),\n ('busy', 4.80178401692393),\n ('taught', 4.80178401692393),\n ('shut', 4.80178401692393),\n ('boat', 4.80178401692393),\n ('wall', 4.80178401692393),\n ('spend', 5.80178401692393),\n ('summer', 5.80178401692393),\n ('ambushed', 5.80178401692393),\n ('scams', 5.80178401692393),\n ('nuts', 5.80178401692393),\n ('mountain', 5.80178401692393),\n ('thrown', 5.80178401692393),\n ('technically', 5.80178401692393),\n ('house', 5.80178401692393),\n ('fleet', 5.80178401692393),\n ('mention', 5.80178401692393),\n ('cell', 5.80178401692393),\n ('state', 5.80178401692393),\n ('starting', 5.80178401692393),\n ('lied', 5.80178401692393),\n ('cares', 7.025685102665476),\n ('counting', 7.025685102665476),\n ('girlfriend', 7.025685102665476),\n ('either', 7.025685102665476),\n ('cliff', 7.025685102665476),\n ('winning', 7.025685102665476),\n ('yip', 7.025685102665476),\n ('sky', 7.025685102665476),\n ('tile', 7.025685102665476),\n ('swim', 7.025685102665476),\n ('duke', 7.025685102665476),\n ('draw', 7.025685102665476),\n ('western', 7.025685102665476),\n ('expert', 7.025685102665476),\n ('jins', 7.025685102665476),\n ('patient', 7.025685102665476),\n ('orb', 7.025685102665476),\n ('shoes', 7.025685102665476),\n ('fool', 7.025685102665476),\n ('notice', 7.025685102665476),\n ('brain', 7.025685102665476),\n ('glow', 7.025685102665476),\n ('keeps', 7.025685102665476),\n ('suspect', 8.60356803384786),\n ('sparky', 8.60356803384786),\n ('chase', 8.60356803384786),\n ('relying', 8.60356803384786),\n ('crucial', 8.60356803384786),\n ('parties', 8.60356803384786),\n ('oof', 8.60356803384786),\n ('shift', 8.60356803384786),\n ('explained', 8.60356803384786),\n ('useless', 8.60356803384786),\n ('fathers', 8.60356803384786),\n ('owner', 8.60356803384786),\n ('possibilities', 8.60356803384786),\n ('range', 8.60356803384786),\n ('flare', 8.60356803384786),\n ('sewing', 8.60356803384786),\n ('trading', 8.60356803384786),\n ('nature', 8.60356803384786),\n ('snuff', 8.60356803384786),\n ('teenager', 8.60356803384786),\n ('selfish', 8.60356803384786),\n ('sapphire', 8.60356803384786),\n ('failure', 8.60356803384786),\n ('er', 8.60356803384786),\n ('classic', 8.60356803384786),\n ('neither', 8.60356803384786),\n ('struck', 8.60356803384786),\n ('discovers', 8.60356803384786),\n ('labyrinth', 8.60356803384786),\n ('agree', 8.60356803384786),\n ('lovely', 8.60356803384786),\n ('tall', 8.60356803384786),\n ('definitely', 8.60356803384786),\n ('rise', 8.60356803384786),\n ('cheered', 8.60356803384786),\n ('distracted', 8.60356803384786),\n ('remote', 8.60356803384786),\n ('celebrate', 8.60356803384786),\n ('ashamed', 8.60356803384786),\n ('celestial', 8.60356803384786),\n ('regain', 8.60356803384786),\n ('directly', 8.60356803384786),\n ('everyday', 8.60356803384786),\n ('pushover', 10.827469119589407),\n ('songs', 10.827469119589407),\n ('rebirth', 10.827469119589407),\n ('diddly', 10.827469119589407),\n ('rush', 10.827469119589407),\n ('sweetness', 10.827469119589407),\n ('chit', 10.827469119589407),\n ('aangy', 10.827469119589407),\n ('zu', 10.827469119589407),\n ('strangest', 10.827469119589407),\n ('dual', 10.827469119589407),\n ('dee', 10.827469119589407),\n ('uphill', 10.827469119589407),\n ('reasons', 10.827469119589407),\n ('cleans', 10.827469119589407),\n ('luxury', 10.827469119589407),\n ('inescapable', 10.827469119589407),\n ('uniforms', 10.827469119589407),\n ('blowing', 10.827469119589407),\n ('yards', 10.827469119589407),\n ('loud', 10.827469119589407),\n ('hue', 10.827469119589407),\n ('mysteries', 10.827469119589407),\n ('individual', 10.827469119589407),\n ('detonates', 10.827469119589407),\n ('dense', 10.827469119589407),\n ('performers', 10.827469119589407),\n ('threat', 10.827469119589407),\n ('swine', 10.827469119589407),\n ('ashes', 10.827469119589407),\n ('stowaways', 10.827469119589407),\n ('weary', 10.827469119589407),\n ('blisters', 10.827469119589407),\n ('behold', 10.827469119589407),\n ('earthiness', 10.827469119589407),\n ('incredibly', 10.827469119589407),\n ('marred', 10.827469119589407),\n ('acquainted', 10.827469119589407),\n ('savages', 10.827469119589407),\n ('appetite', 10.827469119589407),\n ('arrrr', 10.827469119589407),\n ('inward', 10.827469119589407),\n ('mortal', 10.827469119589407),\n ('nailed', 10.827469119589407),\n ('yangchen', 10.827469119589407),\n ('ditching', 10.827469119589407),\n ('trial', 10.827469119589407),\n ('harmony', 10.827469119589407),\n ('abuse', 10.827469119589407),\n ('chong', 10.827469119589407),\n ('connecting', 10.827469119589407),\n ('shocked', 10.827469119589407),\n ('tricked', 10.827469119589407),\n ('fin', 10.827469119589407),\n ('talented', 10.827469119589407),\n ('tenth', 10.827469119589407),\n ('adventures', 10.827469119589407),\n ('whhhoa', 10.827469119589407),\n ('award', 10.827469119589407),\n ('scour', 10.827469119589407),\n ('thirst', 10.827469119589407),\n ('gas', 10.827469119589407),\n ('jeez', 10.827469119589407),\n ('married', 10.827469119589407),\n ('ehh', 10.827469119589407),\n ('governor', 10.827469119589407),\n ('tightly', 10.827469119589407),\n ('dam', 10.827469119589407),\n ('alarm', 10.827469119589407),\n ('paying', 10.827469119589407),\n ('suffered', 10.827469119589407),\n ('masks', 10.827469119589407),\n ('ridding', 10.827469119589407),\n ('oval', 10.827469119589407),\n ('darn', 10.827469119589407),\n ('beneath', 10.827469119589407),\n ('switched', 10.827469119589407),\n ('meatheads', 10.827469119589407),\n ('ughh', 10.827469119589407),\n ('cleaned', 10.827469119589407),\n ('bean', 10.827469119589407),\n ('mechanist', 10.827469119589407),\n ('blaugh', 10.827469119589407),\n ('kindling', 10.827469119589407),\n ('sink', 10.827469119589407),\n ('feminine', 10.827469119589407),\n ('scamming', 10.827469119589407),\n ('lilies', 10.827469119589407),\n ('characters', 10.827469119589407),\n ('checks', 10.827469119589407),\n ('bathhouse', 10.827469119589407),\n ('obedient', 10.827469119589407),\n ('torpedo', 10.827469119589407),\n ('fatso', 10.827469119589407),\n ('trips', 10.827469119589407),\n ('pampering', 10.827469119589407),\n ('wings', 10.827469119589407),\n ('somethin', 10.827469119589407),\n ('melt', 10.827469119589407),\n ('traumatic', 10.827469119589407),\n ('pride', 10.827469119589407),\n ('compassionate', 10.827469119589407),\n ('overslept', 10.827469119589407),\n ('volcanic', 10.827469119589407),\n ('resources', 10.827469119589407),\n ('woodwinds', 10.827469119589407),\n ('predicted', 10.827469119589407),\n ('bracing', 10.827469119589407),\n ('common', 10.827469119589407),\n ('peek', 10.827469119589407),\n ('sport', 10.827469119589407),\n ('wrongfully', 10.827469119589407),\n ('sticks', 10.827469119589407),\n ('hygiene', 10.827469119589407),\n ('aaaahhhh', 10.827469119589407),\n ('backward', 10.827469119589407),\n ('initiate', 10.827469119589407),\n ('continues', 10.827469119589407),\n ('dated', 10.827469119589407),\n ('servants', 10.827469119589407),\n ('lowlife', 10.827469119589407),\n ('lychee', 10.827469119589407),\n ('surfing', 10.827469119589407),\n ('formal', 10.827469119589407)]"
     },
     "metadata": {},
     "execution_count": 278
    }
   ],
   "source": [
    "sorted(idf.items(), key=lambda x: x[1])[::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_set = major_characters\n",
    "character_bows = season_bows['all']\n",
    "all_words = set(k for bow in character_bows.values() for k in bow.keys())\n",
    "n_chars = len(character_set)\n",
    "idf = {\n",
    "    w: 1 + log(n_chars / (1 + df), base)\n",
    "    for w in all_words if (df := sum(character_bows[c][w] > 0 for c in character_set)) > 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "120138"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "sum(total_word_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "season_bows['all']['sokka']['weirdness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('scare', 1.0),\n ('cookin', 1.0),\n ('weirdness', 1.0),\n ('steering', 1.0),\n ('waterbended', 1.0),\n ('screw', 1.0),\n ('freakish', 1.0),\n ('ewww', 1.0),\n ('aahh', 1.0),\n ('bolt', 1.0),\n ('sneezed', 1.0),\n ('beams', 1.0),\n ('snot', 1.0),\n ('uuuuugh', 1.0),\n ('stab', 1.0)]"
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "season_bayes['all']['sokka'][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Azula!'),\n ({'neg': 0.0, 'neu': 0.598, 'pos': 0.402, 'compound': 0.6166},\n  ' It is so good to see you!'),\n ({'neg': 0.0, 'neu': 0.312, 'pos': 0.688, 'compound': 0.296}, ' Oh yeah.'),\n ({'neg': 0.0, 'neu': 0.445, 'pos': 0.555, 'compound': 0.5777},\n  'He was so funny.'),\n ({'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'compound': 0.6369},\n  ' Oh...I...uh...would love to.'),\n ({'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'compound': 0.7425},\n  \"But the truth is, I'm really happy here.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'I mean, my aura has never been pinker!'),\n ({'neg': 0.0, 'neu': 0.444, 'pos': 0.556, 'compound': 0.3612},\n  ' Thank you, Azula.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' Uh...yeah...sure...uh...of course...'),\n ({'neg': 0.347, 'neu': 0.653, 'pos': 0.0, 'compound': -0.5023},\n  \" I'm sorry Azula, but unfortunately there won't be a show tomorrow.\"),\n ({'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'compound': 0.5106},\n  \" The universe has given me strong hints that it's time for a career change.\"),\n ({'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.3612},\n  'I want to join you on your mission.'),\n ({'neg': 0.0, 'neu': 0.656, 'pos': 0.344, 'compound': 0.2732},\n  ' Well, Azula called a little louder.'),\n ({'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'compound': 0.4019},\n  \" It'll be interesting seeing Zuko again, won't it, Mai?\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Azula!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Hmm...'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"They're not wads, more like... bundles.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, 'Or bunches...'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'It\\'s got an \"uh\" sound.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Clumps!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, \"They're clumps!\"),\n ({'neg': 0.0, 'neu': 0.769, 'pos': 0.231, 'compound': 0.4033},\n  ' Was it just me, or was that guy kinda cute?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, '  Hmm.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'What about those muscle-y guys down there?'),\n ({'neg': 0.0, 'neu': 0.662, 'pos': 0.338, 'compound': 0.6239},\n  ' Wow Azula, you were right, it is the Avatar!'),\n ({'neg': 0.0, 'neu': 0.392, 'pos': 0.608, 'compound': 0.4767},\n  '... and friends.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Oh, I get it!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, 'I get it.'),\n ({'neg': 0.0, 'neu': 0.385, 'pos': 0.615, 'compound': 0.4926},\n  'Good one, Azula!'),\n ({'neg': 0.0, 'neu': 0.541, 'pos': 0.459, 'compound': 0.5255},\n  \" You're prettier than we are!\"),\n ({'neg': 0.0, 'neu': 0.789, 'pos': 0.211, 'compound': 0.3401},\n  \" Mai finally gets to wear makeup that's not totally depressing?\"),\n ({'neg': 0.0, 'neu': 0.422, 'pos': 0.578, 'compound': 0.6269},\n  \" Gosh, you're so confident.\"),\n ({'neg': 0.0, 'neu': 0.541, 'pos': 0.459, 'compound': 0.5256},\n  'I really admire that about you.'),\n ({'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.5777},\n  \" Maybe that's why it was so easy to beat the Kyoshi Warriors and take their clothes.\"),\n ({'neg': 0.115, 'neu': 0.766, 'pos': 0.12, 'compound': 0.0258},\n  ' Princess Azula promised we would go back to the Fire Nation as soon as we captured the Avatar.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'We just have to be patient.'),\n ({'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.0772}, ' Sorry.'),\n ({'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.4215},\n  ' Nice speech Azula.'),\n ({'neg': 0.236, 'neu': 0.438, 'pos': 0.326, 'compound': 0.1655},\n  'It was pretty and poetic, but also scary in a good way.'),\n ({'neg': 0.0, 'neu': 0.561, 'pos': 0.439, 'compound': 0.5719},\n  ' Yeah, what are you in time for... cutie?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Who?'),\n ({'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612},\n  \" Ooo, it's like we're dancing together.\"),\n ({'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404},\n  \" Come on, it's easy.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'You just walk on your front paws instead of your rear ones.'),\n ({'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'compound': 0.3612}, 'Like this.'),\n ({'neg': 0.0, 'neu': 0.77, 'pos': 0.23, 'compound': 0.4005},\n  \" I'm so excited to spend the weekend on Ember Island.\"),\n ({'neg': 0.0, 'neu': 0.745, 'pos': 0.255, 'compound': 0.6249},\n  \"It's gonna be great to hang out on the beach and do nothing.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"  Doesn't your family have a house on Ember Island?\"),\n ({'neg': 0.0, 'neu': 0.455, 'pos': 0.545, 'compound': 0.5562},\n  \" That must've been fun!\"),\n ({'neg': 0.0, 'neu': 0.562, 'pos': 0.438, 'compound': 0.5994},\n  ' Who are these two beautiful women?'),\n ({'neg': 0.0, 'neu': 0.471, 'pos': 0.529, 'compound': 0.6696},\n  ' Ooh, I love this seashell bedspread!'),\n ({'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6369}, ' Sure, thanks.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' Could you scooch just a little bit more to the...'),\n ({'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}, 'Perfect.'),\n ({'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.7717}, ' Wow, thanks.'),\n ({'neg': 0.0, 'neu': 0.422, 'pos': 0.578, 'compound': 0.6269},\n  'This is so pretty.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' Ahem, shade... shade!'),\n ({'neg': 0.0, 'neu': 0.122, 'pos': 0.878, 'compound': 0.8016},\n  ' Sure...I love parties!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' They should.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" I don't know, I don't know.\"),\n ({'neg': 0.0, 'neu': 0.571, 'pos': 0.429, 'compound': 0.4588},\n  \" Oh, I'm glad you're here.\"),\n ({'neg': 0.0, 'neu': 0.581, 'pos': 0.419, 'compound': 0.2235},\n  \"Those boys won't leave me alone.\"),\n ({'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.3612},\n  'I guess they all just like me too much.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' What are you talking about?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' What?'),\n ({'neg': 0.429, 'neu': 0.571, 'pos': 0.0, 'compound': -0.4588},\n  'You were jealous of me?'),\n ({'neg': 0.0, 'neu': 0.344, 'pos': 0.656, 'compound': 0.9645},\n  \"Um, but, you're the most beautiful, smartest, perfect girl in the world.\"),\n ({'neg': 0.304, 'neu': 0.696, 'pos': 0.0, 'compound': -0.5423},\n  ' But you probably would do something horrible to them.'),\n ({'neg': 0.284, 'neu': 0.49, 'pos': 0.225, 'compound': -0.1531},\n  \"I'm sure they're just intimidated by you.\"),\n ({'neg': 0.068, 'neu': 0.591, 'pos': 0.341, 'compound': 0.8269},\n  \"Ok, look, if you want a boy to like you just look at him and smile a lot and laugh at everything he says, even if it's not funny.\"),\n ({'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.6369},\n  ' Ok. \"Hey, there sweet sugar cakes.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'How ya likin\\' this party?\"'),\n ({'neg': 0.583, 'neu': 0.417, 'pos': 0.0, 'compound': -0.1027},\n  \" I'm freezing.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' What are you doing?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" But it's a painting of your family.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' I think you do.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' I know you.'),\n ({'neg': 0.382, 'neu': 0.263, 'pos': 0.355, 'compound': -0.0516},\n  \" Yes, I'm a circus freak.\"),\n ({'neg': 0.0, 'neu': 0.505, 'pos': 0.495, 'compound': 0.5994},\n  'Go ahead and laugh all you want.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'You wanna know why I joined the circus?'),\n ({'neg': 0.0, 'neu': 0.729, 'pos': 0.271, 'compound': 0.6908},\n  ' Do you have any idea what my home life was like, growing up with six sisters who look exactly like me?'),\n ({'neg': 0.0, 'neu': 0.762, 'pos': 0.238, 'compound': 0.3612},\n  \"It was like, I didn't even have my own name.\"),\n ({'neg': 0.146, 'neu': 0.854, 'pos': 0.0, 'compound': -0.4404},\n  'I joined the circus because I was scared of spending the rest of my life as part of a matched set.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"At least, I'm different now.\"),\n ({'neg': 0.35, 'neu': 0.241, 'pos': 0.409, 'compound': 0.126},\n  'Circus freak is a compliment!'),\n ({'neg': 0.394, 'neu': 0.606, 'pos': 0.0, 'compound': -0.0772},\n  \" I'm sorry, what?\"),\n ({'neg': 0.0, 'neu': 0.469, 'pos': 0.531, 'compound': 0.34},\n  \" Well, what's your excuse, Mai?\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'You were an only child for 15 years, but even with all that attention, your aura is this dingy, pasty, gray...'),\n ({'neg': 0.0, 'neu': 0.566, 'pos': 0.434, 'compound': 0.3182},\n  ' Calm down, you guys.'),\n ({'neg': 0.471, 'neu': 0.392, 'pos': 0.137, 'compound': -0.7269},\n  'This much negative energy is bad for your skin.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"You'll totally break out.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Your uncle?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Talk to us.'),\n ({'neg': 0.0, 'neu': 0.682, 'pos': 0.318, 'compound': 0.4215},\n  ' What Lo and Li said came true.'),\n ({'neg': 0.0, 'neu': 0.722, 'pos': 0.278, 'compound': 0.4019},\n  'The beach did help us learn about ourselves.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'I feel all smoothed.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"I'll always remember this.\"),\n ({'neg': 0.324, 'neu': 0.676, 'pos': 0.0, 'compound': -0.3382},\n  \" They're about to cut the line!\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" Come on,  let's get out of here!\")]"
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "char_sentence_sentiments['ty lee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "defaultdict(<function __main__.get_characters_and_sentiment_frequencies.<locals>.<lambda>()>,\n            {'sokka': defaultdict(int,\n                         {'neu': 0.5657534246575499,\n                          'neg': 0.20410958904109833,\n                          'pos': 0.23013698630137325}),\n             'katara': defaultdict(int,\n                         {'neu': 0.5485395189003418,\n                          'neg': 0.19802405498281725,\n                          'pos': 0.2534364261168377}),\n             'zuko': defaultdict(int,\n                         {'neu': 0.5496872828352996,\n                          'pos': 0.24252953439888933,\n                          'neg': 0.20778318276581034}),\n             'iroh': defaultdict(int,\n                         {'neu': 0.43678160919540476,\n                          'pos': 0.37356321839080653,\n                          'neg': 0.1896551724137935}),\n             'aang': defaultdict(int,\n                         {'neu': 0.573233959818546,\n                          'neg': 0.160401814646791,\n                          'pos': 0.26636422553466904}),\n             'zhao': defaultdict(int,\n                         {'neu': 0.4618834080717498,\n                          'pos': 0.26008968609865507,\n                          'neg': 0.2780269058295968}),\n             'momo': defaultdict(int,\n                         {'neu': 0.6000000000000001, 'neg': 0.2, 'pos': 0.2}),\n             'suki': defaultdict(int,\n                         {'neu': 0.4818652849740939,\n                          'neg': 0.20207253886010354,\n                          'pos': 0.3160621761658032}),\n             'bumi': defaultdict(int,\n                         {'neu': 0.5199999999999995,\n                          'pos': 0.3266666666666663,\n                          'neg': 0.1533333333333333}),\n             'warden': defaultdict(int,\n                         {'pos': 0.26363636363636367,\n                          'neu': 0.45454545454545464,\n                          'neg': 0.28181818181818186}),\n             'appa': defaultdict(int,\n                         {'neu': 0.42857142857142855,\n                          'pos': 0.14285714285714285,\n                          'neg': 0.42857142857142855}),\n             'roku': defaultdict(int,\n                         {'pos': 0.3666666666666667,\n                          'neu': 0.3777777777777778,\n                          'neg': 0.2555555555555554}),\n             'jet': defaultdict(int,\n                         {'neu': 0.5148514851485152,\n                          'pos': 0.2524752475247526,\n                          'neg': 0.2326732673267328}),\n             'ozai': defaultdict(int,\n                         {'pos': 0.2956521739130436,\n                          'neg': 0.2434782608695653,\n                          'neu': 0.46086956521739153}),\n             'hakoda': defaultdict(int,\n                         {'neu': 0.4927536231884051,\n                          'neg': 0.22463768115942037,\n                          'pos': 0.2826086956521739}),\n             'bato': defaultdict(int,\n                         {'neu': 0.5227272727272725,\n                          'pos': 0.284090909090909,\n                          'neg': 0.19318181818181815}),\n             'jeong jeong': defaultdict(int,\n                         {'neu': 0.5121951219512197,\n                          'neg': 0.28048780487804886,\n                          'pos': 0.20731707317073175}),\n             'the mechanist': defaultdict(int,\n                         {'neu': 0.5876288659793815,\n                          'neg': 0.24742268041237106,\n                          'pos': 0.16494845360824742}),\n             'yue': defaultdict(int,\n                         {'pos': 0.3043478260869564,\n                          'neu': 0.5434782608695651,\n                          'neg': 0.15217391304347822}),\n             'pakku': defaultdict(int,\n                         {'pos': 0.5131578947368417,\n                          'neu': 0.32894736842105243,\n                          'neg': 0.15789473684210525}),\n             'azula': defaultdict(int,\n                         {'neg': 0.22515212981744487,\n                          'neu': 0.4726166328600429,\n                          'pos': 0.3022312373225164}),\n             'chong': defaultdict(int,\n                         {'neg': 0.17910447761194026,\n                          'neu': 0.5373134328358207,\n                          'pos': 0.2835820895522387}),\n             'mai': defaultdict(int,\n                         {'neg': 0.2769230769230768,\n                          'neu': 0.46923076923076845,\n                          'pos': 0.25384615384615383}),\n             'ty lee': defaultdict(int,\n                         {'neu': 0.39393939393939414,\n                          'pos': 0.49494949494949525,\n                          'neg': 0.1111111111111111}),\n             'toph': defaultdict(int,\n                         {'neg': 0.18171021377672242,\n                          'neu': 0.5273159144893126,\n                          'pos': 0.29097387173396744}),\n             'professor zei': defaultdict(int,\n                         {'pos': 0.3278688524590164,\n                          'neu': 0.6065573770491807,\n                          'neg': 0.06557377049180328}),\n             'joo dee': defaultdict(int,\n                         {'neu': 0.5517241379310344,\n                          'pos': 0.2931034482758621,\n                          'neg': 0.15517241379310348}),\n             'long feng': defaultdict(int,\n                         {'pos': 0.2526315789473685,\n                          'neu': 0.42105263157894707,\n                          'neg': 0.3263157894736841}),\n             'earth king': defaultdict(int,\n                         {'neg': 0.25757575757575746,\n                          'neu': 0.3939393939393937,\n                          'pos': 0.3484848484848483}),\n             'guru pathik': defaultdict(int,\n                         {'neu': 0.41237113402061876,\n                          'pos': 0.3711340206185568,\n                          'neg': 0.2164948453608247}),\n             'piandao': defaultdict(int,\n                         {'neu': 0.5569620253164559,\n                          'pos': 0.30379746835443056,\n                          'neg': 0.13924050632911394}),\n             'hama': defaultdict(int,\n                         {'neg': 0.19191919191919196,\n                          'pos': 0.2727272727272728,\n                          'neu': 0.5353535353535355})})"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "get_characters_and_sentiment_frequencies(char_sentence_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('sokka', 53.33576813857113),\n ('unagi', 51.94751862035726),\n ('warrior', 47.944778895405705),\n ('gondola', 41.91093093472921),\n ('safely', 40.21778375416927),\n ('sorry', 39.308849623729316),\n ('warden', 39.30194181625914),\n ('tickets', 38.786248210482114),\n ('mean', 37.49031541235173),\n ('guys', 37.203329527591016),\n ('big', 35.61735676579326),\n ('fly', 35.473951045376594),\n ('huh', 34.893509477357604),\n ('help', 34.79479532095062)]"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "season_tfidf['all']['suki'][1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('sokka', 19024),\n ('aang', 18846),\n ('katara', 15465),\n ('zuko', 9693),\n ('toph', 5675),\n ('iroh', 5437),\n ('azula', 3799),\n ('zhao', 1678),\n ('jet', 1444),\n ('suki', 1278),\n ('hakoda', 1105),\n ('bumi', 1045),\n ('ozai', 952),\n ('hama', 936),\n ('guru pathik', 862),\n ('mai', 844),\n ('roku', 806),\n ('long feng', 776),\n ('warden', 750),\n ('bato', 696),\n ('piandao', 689),\n ('the mechanist', 682),\n ('ty lee', 675),\n ('pakku', 587),\n ('yue', 578),\n ('jeong jeong', 562),\n ('chong', 533),\n ('earth king', 520),\n ('joo dee', 489),\n ('zhang leader', 469),\n ('old man', 456),\n ('dock', 454),\n ('professor zei', 439),\n ('teo', 392),\n ('chief arnook', 372),\n ('general fong', 370),\n ('aunt wu', 358),\n ('guard', 358),\n ('gan jin leader', 356),\n ('sun warrior chief', 344)]"
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "# Sort by total word count\n",
    "sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('sokka', 7787),\n ('aang', 7445),\n ('katara', 6061),\n ('zuko', 3650),\n ('iroh', 2355),\n ('toph', 2199),\n ('azula', 1552),\n ('zhao', 729),\n ('jet', 568),\n ('suki', 482),\n ('hakoda', 473),\n ('bumi', 464),\n ('ozai', 390),\n ('hama', 387),\n ('guru pathik', 381),\n ('mai', 351),\n ('roku', 350),\n ('long feng', 345),\n ('the mechanist', 327),\n ('piandao', 305),\n ('warden', 300),\n ('bato', 279),\n ('ty lee', 272),\n ('pakku', 265),\n ('jeong jeong', 250),\n ('zhang leader', 231),\n ('earth king', 229),\n ('chong', 216),\n ('joo dee', 212),\n ('professor zei', 211),\n ('yue', 205),\n ('old man', 189),\n ('dock', 187),\n ('chief arnook', 178),\n ('gan jin leader', 169),\n ('general fong', 160),\n ('shyu', 157),\n ('teo', 155),\n ('aunt wu', 152),\n ('sun warrior chief', 147)]"
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "# Sort by total non-stopword word count\n",
    "sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := sum(season_bows['all'][char].values()) - season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38364bitavatarscriptanalysispipenv8de1ba2f463a4d24ae2f4934f1108141",
   "display_name": "Python 3.8.3 64-bit ('avatar-script-analysis': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}