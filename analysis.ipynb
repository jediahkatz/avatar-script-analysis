{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_characters = {\n",
    "    'sokka', 'aang', 'katara', 'zuko', 'toph', 'iroh', 'zuko', 'azula', # 1000+ non-stopwords\n",
    "    'zhao', 'jet', 'suki', 'hakoda', 'bumi',                            # 400+ non-stopwords\n",
    "}\n",
    "minor_characters = {\n",
    "    'hama', 'ozai', 'guru pathik', 'mai', 'roku', 'long feng',          # 350+ non-stopwords\n",
    "    'the mechanist', 'piandao', 'warden', 'bato', 'ty lee',\n",
    "    'jeong jeong', 'pakku', 'zhang leader', 'earth king',\n",
    "    'professor zei', 'joo dee', 'chong', 'yue',                         # 200+ non-stopwords\n",
    "    'appa', 'momo'                                                      # :)\n",
    "}\n",
    "aliases = {\n",
    "    'fire lord ozai': 'ozai', 'ruko': 'roku', 'princess yue': 'yue', 'princess yue / moon spirit': 'yue', 'monk gyatso': 'gyatso',\n",
    "    'themechanist': 'the mechanist', 'zuko/blue spirit': 'zuko', 'king bumi': 'bumi', 'master pakku': 'pakku'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def get_ep_character_bows(script_lines):\n",
    "    \"\"\"Return a set of bags of words for this episode as a mapping from characters to words to counts.\"\"\"\n",
    "    bow = defaultdict(Counter)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        # if speaker not in important_characters:\n",
    "        #     continue\n",
    "        line = line.lower()\n",
    "        # Get all non-stop words\n",
    "        stop_words = set(w for w in stopwords.words('english') \n",
    "            + ['us', 'get', 'like', 'thats', 'go', 'going', 'cant', 'yeh', 'oh', 'got', 'hey', \n",
    "               'yeah', 'uh', 'whats', 'could', 'shall', 'gonna', 'okay', 'one'] \n",
    "        )\n",
    "        words = re.findall(r'\\w+', line)\n",
    "        bow[speaker]['total_count'] += len(words)\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        bow[speaker].update(words)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ep_total_word_count(script_lines):\n",
    "    return sum(len(re.findall(r'\\w+', line.split('::')[1])) for line in script_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_bows(bows):\n",
    "    \"\"\"Merge a collection of bags of words into one. Does not modify the input.\"\"\"\n",
    "    combined_bow = Counter()\n",
    "    for bow in bows:\n",
    "        combined_bow.update(bow)\n",
    "    return combined_bow\n",
    "\n",
    "def combine_character_bows(character_bows):\n",
    "    \"\"\"Merge a collection of mappings from characters to bags of words into one. Does not modify the input.\"\"\"\n",
    "    all_characters = set(k for bow in character_bows for k in bow.keys())\n",
    "    combined_bow_set = {c: combine_bows(bow_map[c] for bow_map in character_bows) for c in all_characters}\n",
    "    return defaultdict(Counter, combined_bow_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is a bad metric -- it just finds words that this character said once and nobody else ever said.\n",
    "def get_words_by_bayes(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, p), where w is a word \n",
    "    and p is the probability that a random occurrence of the word was spoken by that character.\n",
    "    The lists are sorted by descending probability.\n",
    "    \"\"\"\n",
    "    # Compute the total number of occurrences for each word\n",
    "    combined_bow = combine_bows(character_bows.values())\n",
    "    # p(character | word) = [# times character says word] / [# occurrences of word]\n",
    "    words_by_bayes = {\n",
    "        c: sorted(\n",
    "            [(w, character_bows[c][w] / combined_bow[w]) for w in combined_bow.keys()],\n",
    "            key=lambda wp: wp[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "# Since we have very few characters, we want to weight idf highly (with a lower base)\n",
    "base = 1.2\n",
    "\n",
    "def get_words_by_tfidf(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, s), where w is a word \n",
    "    and s is the tf-idf score of that word for that character. The lists are sorted \n",
    "    by descending score.\n",
    "    \"\"\"\n",
    "    # Compute the number of characters that said each word\n",
    "    all_words = set(k for bow in character_bows.values() for k in bow.keys())\n",
    "    n_chars = len(character_bows.keys())\n",
    "    idf = {\n",
    "        w: 1 + log(n_chars / (1 + sum(character_bows[c][w] > 0 for c in character_bows.keys())), base)\n",
    "        for w in all_words\n",
    "    }\n",
    "    # tf-idf = tf(w, c) / (1 + log(N / (1 + df(w))))\n",
    "    words_by_tfidf = {\n",
    "        c: sorted(\n",
    "            [(w, (1 + v) * idf[w]) for w, v in character_bows[c].items()],\n",
    "            key=lambda ws: ws[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_character_sentence_sentiments(script_lines):\n",
    "    \"\"\"\n",
    "    Return a mapping from each character to a list of all their sentences along with the\n",
    "    computed sentiment scores (positive, negative, neutral) for each sentence.\n",
    "    \"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    char_sentence_sentiments = defaultdict(list)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        if speaker not in important_characters | minor_characters:\n",
    "            continue\n",
    "        \n",
    "        char_sentence_sentiments[speaker].extend(\n",
    "            (sid.polarity_scores(sentence), sentence) for sentence in tokenizer.tokenize(line)\n",
    "        )\n",
    "\n",
    "    return char_sentence_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_character_sentence_sentiments(target_map, added_map):\n",
    "    \"\"\"\n",
    "    Given two maps from characters to lists of sentence sentiments, combine all lists with the same key.\n",
    "    The first argument will be modified.\n",
    "    \"\"\"\n",
    "    for char in added_map.keys():\n",
    "        target_map[char].extend(added_map[char])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters_and_sentiment_frequencies(char_sentence_sentiments):\n",
    "    \"\"\"\n",
    "    Given a map from characters to lists of sentence sentiments, return a map\n",
    "    from characters to lists of the frequency of positive, negative, and neutral\n",
    "    sentiments.\n",
    "    \"\"\"\n",
    "    sentiment_freqs = defaultdict(lambda: defaultdict(int))\n",
    "    for char, sentiments in char_sentence_sentiments.items():\n",
    "        for (scores, _) in sentiments:\n",
    "            sentiment_freqs[char][\n",
    "                'pos' if scores['compound'] > 0.05 else 'neg' if scores['compound'] < -0.05 else 'neu'\n",
    "            ] += 1/len(sentiments)\n",
    "    return sentiment_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "season_bows_collection = defaultdict(list)\n",
    "season_bows = {}\n",
    "season_tfidf = {}\n",
    "season_bayes = {}\n",
    "total_word_count = {}\n",
    "char_sentence_sentiments = defaultdict(list)\n",
    "for season in range(1, 4):\n",
    "    for episode in range(1, 21):\n",
    "        ep_num = f'{season}{str(episode).zfill(2)}'\n",
    "        with open(f'transcripts/{ep_num}.txt') as f:\n",
    "            script_lines = f.readlines()\n",
    "            season_bows_collection[season].append(get_ep_character_bows(script_lines))\n",
    "            merge_character_sentence_sentiments(char_sentence_sentiments, get_character_sentence_sentiments(script_lines))\n",
    "            total_word_count[ep_num] = get_ep_total_word_count(script_lines)\n",
    "    season_bows[season] = combine_character_bows(season_bows_collection[season])\n",
    "    season_tfidf[season] = get_words_by_tfidf(season_bows[season])\n",
    "    season_bayes[season] = get_words_by_bayes(season_bows[season])\n",
    "season_bows['all'] = combine_character_bows([bows for s in range(1, 4) for bows in season_bows_collection[s]])\n",
    "season_tfidf['all'] = get_words_by_tfidf(season_bows['all'])\n",
    "season_bayes['all'] = get_words_by_bayes(season_bows['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "120138"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "sum(total_word_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "season_bows['all']['sokka']['weirdness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('scare', 1.0),\n ('cookin', 1.0),\n ('weirdness', 1.0),\n ('steering', 1.0),\n ('waterbended', 1.0),\n ('screw', 1.0),\n ('freakish', 1.0),\n ('ewww', 1.0),\n ('aahh', 1.0),\n ('bolt', 1.0),\n ('sneezed', 1.0),\n ('beams', 1.0),\n ('snot', 1.0),\n ('uuuuugh', 1.0),\n ('stab', 1.0)]"
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "season_bayes['all']['sokka'][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Azula!'),\n ({'neg': 0.0, 'neu': 0.598, 'pos': 0.402, 'compound': 0.6166},\n  ' It is so good to see you!'),\n ({'neg': 0.0, 'neu': 0.312, 'pos': 0.688, 'compound': 0.296}, ' Oh yeah.'),\n ({'neg': 0.0, 'neu': 0.445, 'pos': 0.555, 'compound': 0.5777},\n  'He was so funny.'),\n ({'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'compound': 0.6369},\n  ' Oh...I...uh...would love to.'),\n ({'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'compound': 0.7425},\n  \"But the truth is, I'm really happy here.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'I mean, my aura has never been pinker!'),\n ({'neg': 0.0, 'neu': 0.444, 'pos': 0.556, 'compound': 0.3612},\n  ' Thank you, Azula.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' Uh...yeah...sure...uh...of course...'),\n ({'neg': 0.347, 'neu': 0.653, 'pos': 0.0, 'compound': -0.5023},\n  \" I'm sorry Azula, but unfortunately there won't be a show tomorrow.\"),\n ({'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'compound': 0.5106},\n  \" The universe has given me strong hints that it's time for a career change.\"),\n ({'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.3612},\n  'I want to join you on your mission.'),\n ({'neg': 0.0, 'neu': 0.656, 'pos': 0.344, 'compound': 0.2732},\n  ' Well, Azula called a little louder.'),\n ({'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'compound': 0.4019},\n  \" It'll be interesting seeing Zuko again, won't it, Mai?\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Azula!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Hmm...'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"They're not wads, more like... bundles.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, 'Or bunches...'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'It\\'s got an \"uh\" sound.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Clumps!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, \"They're clumps!\"),\n ({'neg': 0.0, 'neu': 0.769, 'pos': 0.231, 'compound': 0.4033},\n  ' Was it just me, or was that guy kinda cute?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, '  Hmm.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'What about those muscle-y guys down there?'),\n ({'neg': 0.0, 'neu': 0.662, 'pos': 0.338, 'compound': 0.6239},\n  ' Wow Azula, you were right, it is the Avatar!'),\n ({'neg': 0.0, 'neu': 0.392, 'pos': 0.608, 'compound': 0.4767},\n  '... and friends.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Oh, I get it!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, 'I get it.'),\n ({'neg': 0.0, 'neu': 0.385, 'pos': 0.615, 'compound': 0.4926},\n  'Good one, Azula!'),\n ({'neg': 0.0, 'neu': 0.541, 'pos': 0.459, 'compound': 0.5255},\n  \" You're prettier than we are!\"),\n ({'neg': 0.0, 'neu': 0.789, 'pos': 0.211, 'compound': 0.3401},\n  \" Mai finally gets to wear makeup that's not totally depressing?\"),\n ({'neg': 0.0, 'neu': 0.422, 'pos': 0.578, 'compound': 0.6269},\n  \" Gosh, you're so confident.\"),\n ({'neg': 0.0, 'neu': 0.541, 'pos': 0.459, 'compound': 0.5256},\n  'I really admire that about you.'),\n ({'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.5777},\n  \" Maybe that's why it was so easy to beat the Kyoshi Warriors and take their clothes.\"),\n ({'neg': 0.115, 'neu': 0.766, 'pos': 0.12, 'compound': 0.0258},\n  ' Princess Azula promised we would go back to the Fire Nation as soon as we captured the Avatar.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'We just have to be patient.'),\n ({'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.0772}, ' Sorry.'),\n ({'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.4215},\n  ' Nice speech Azula.'),\n ({'neg': 0.236, 'neu': 0.438, 'pos': 0.326, 'compound': 0.1655},\n  'It was pretty and poetic, but also scary in a good way.'),\n ({'neg': 0.0, 'neu': 0.561, 'pos': 0.439, 'compound': 0.5719},\n  ' Yeah, what are you in time for... cutie?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Who?'),\n ({'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612},\n  \" Ooo, it's like we're dancing together.\"),\n ({'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404},\n  \" Come on, it's easy.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'You just walk on your front paws instead of your rear ones.'),\n ({'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'compound': 0.3612}, 'Like this.'),\n ({'neg': 0.0, 'neu': 0.77, 'pos': 0.23, 'compound': 0.4005},\n  \" I'm so excited to spend the weekend on Ember Island.\"),\n ({'neg': 0.0, 'neu': 0.745, 'pos': 0.255, 'compound': 0.6249},\n  \"It's gonna be great to hang out on the beach and do nothing.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"  Doesn't your family have a house on Ember Island?\"),\n ({'neg': 0.0, 'neu': 0.455, 'pos': 0.545, 'compound': 0.5562},\n  \" That must've been fun!\"),\n ({'neg': 0.0, 'neu': 0.562, 'pos': 0.438, 'compound': 0.5994},\n  ' Who are these two beautiful women?'),\n ({'neg': 0.0, 'neu': 0.471, 'pos': 0.529, 'compound': 0.6696},\n  ' Ooh, I love this seashell bedspread!'),\n ({'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.6369}, ' Sure, thanks.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' Could you scooch just a little bit more to the...'),\n ({'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}, 'Perfect.'),\n ({'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.7717}, ' Wow, thanks.'),\n ({'neg': 0.0, 'neu': 0.422, 'pos': 0.578, 'compound': 0.6269},\n  'This is so pretty.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' Ahem, shade... shade!'),\n ({'neg': 0.0, 'neu': 0.122, 'pos': 0.878, 'compound': 0.8016},\n  ' Sure...I love parties!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' They should.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" I don't know, I don't know.\"),\n ({'neg': 0.0, 'neu': 0.571, 'pos': 0.429, 'compound': 0.4588},\n  \" Oh, I'm glad you're here.\"),\n ({'neg': 0.0, 'neu': 0.581, 'pos': 0.419, 'compound': 0.2235},\n  \"Those boys won't leave me alone.\"),\n ({'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.3612},\n  'I guess they all just like me too much.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' What are you talking about?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' What?'),\n ({'neg': 0.429, 'neu': 0.571, 'pos': 0.0, 'compound': -0.4588},\n  'You were jealous of me?'),\n ({'neg': 0.0, 'neu': 0.344, 'pos': 0.656, 'compound': 0.9645},\n  \"Um, but, you're the most beautiful, smartest, perfect girl in the world.\"),\n ({'neg': 0.304, 'neu': 0.696, 'pos': 0.0, 'compound': -0.5423},\n  ' But you probably would do something horrible to them.'),\n ({'neg': 0.284, 'neu': 0.49, 'pos': 0.225, 'compound': -0.1531},\n  \"I'm sure they're just intimidated by you.\"),\n ({'neg': 0.068, 'neu': 0.591, 'pos': 0.341, 'compound': 0.8269},\n  \"Ok, look, if you want a boy to like you just look at him and smile a lot and laugh at everything he says, even if it's not funny.\"),\n ({'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.6369},\n  ' Ok. \"Hey, there sweet sugar cakes.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'How ya likin\\' this party?\"'),\n ({'neg': 0.583, 'neu': 0.417, 'pos': 0.0, 'compound': -0.1027},\n  \" I'm freezing.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' What are you doing?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" But it's a painting of your family.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' I think you do.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' I know you.'),\n ({'neg': 0.382, 'neu': 0.263, 'pos': 0.355, 'compound': -0.0516},\n  \" Yes, I'm a circus freak.\"),\n ({'neg': 0.0, 'neu': 0.505, 'pos': 0.495, 'compound': 0.5994},\n  'Go ahead and laugh all you want.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'You wanna know why I joined the circus?'),\n ({'neg': 0.0, 'neu': 0.729, 'pos': 0.271, 'compound': 0.6908},\n  ' Do you have any idea what my home life was like, growing up with six sisters who look exactly like me?'),\n ({'neg': 0.0, 'neu': 0.762, 'pos': 0.238, 'compound': 0.3612},\n  \"It was like, I didn't even have my own name.\"),\n ({'neg': 0.146, 'neu': 0.854, 'pos': 0.0, 'compound': -0.4404},\n  'I joined the circus because I was scared of spending the rest of my life as part of a matched set.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"At least, I'm different now.\"),\n ({'neg': 0.35, 'neu': 0.241, 'pos': 0.409, 'compound': 0.126},\n  'Circus freak is a compliment!'),\n ({'neg': 0.394, 'neu': 0.606, 'pos': 0.0, 'compound': -0.0772},\n  \" I'm sorry, what?\"),\n ({'neg': 0.0, 'neu': 0.469, 'pos': 0.531, 'compound': 0.34},\n  \" Well, what's your excuse, Mai?\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'You were an only child for 15 years, but even with all that attention, your aura is this dingy, pasty, gray...'),\n ({'neg': 0.0, 'neu': 0.566, 'pos': 0.434, 'compound': 0.3182},\n  ' Calm down, you guys.'),\n ({'neg': 0.471, 'neu': 0.392, 'pos': 0.137, 'compound': -0.7269},\n  'This much negative energy is bad for your skin.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"You'll totally break out.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Your uncle?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Talk to us.'),\n ({'neg': 0.0, 'neu': 0.682, 'pos': 0.318, 'compound': 0.4215},\n  ' What Lo and Li said came true.'),\n ({'neg': 0.0, 'neu': 0.722, 'pos': 0.278, 'compound': 0.4019},\n  'The beach did help us learn about ourselves.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'I feel all smoothed.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"I'll always remember this.\"),\n ({'neg': 0.324, 'neu': 0.676, 'pos': 0.0, 'compound': -0.3382},\n  \" They're about to cut the line!\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" Come on,  let's get out of here!\")]"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "char_sentence_sentiments['ty lee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "defaultdict(<function __main__.get_characters_and_sentiment_frequencies.<locals>.<lambda>()>,\n            {'sokka': defaultdict(int,\n                         {'neu': 0.5657534246575499,\n                          'neg': 0.20410958904109833,\n                          'pos': 0.23013698630137325}),\n             'katara': defaultdict(int,\n                         {'neu': 0.5485395189003418,\n                          'neg': 0.19802405498281725,\n                          'pos': 0.2534364261168377}),\n             'zuko': defaultdict(int,\n                         {'neu': 0.5496872828352996,\n                          'pos': 0.24252953439888933,\n                          'neg': 0.20778318276581034}),\n             'iroh': defaultdict(int,\n                         {'neu': 0.43678160919540476,\n                          'pos': 0.37356321839080653,\n                          'neg': 0.1896551724137935}),\n             'aang': defaultdict(int,\n                         {'neu': 0.573233959818546,\n                          'neg': 0.160401814646791,\n                          'pos': 0.26636422553466904}),\n             'zhao': defaultdict(int,\n                         {'neu': 0.4618834080717498,\n                          'pos': 0.26008968609865507,\n                          'neg': 0.2780269058295968}),\n             'momo': defaultdict(int,\n                         {'neu': 0.6000000000000001, 'neg': 0.2, 'pos': 0.2}),\n             'suki': defaultdict(int,\n                         {'neu': 0.4818652849740939,\n                          'neg': 0.20207253886010354,\n                          'pos': 0.3160621761658032}),\n             'bumi': defaultdict(int,\n                         {'neu': 0.5199999999999995,\n                          'pos': 0.3266666666666663,\n                          'neg': 0.1533333333333333}),\n             'warden': defaultdict(int,\n                         {'pos': 0.26363636363636367,\n                          'neu': 0.45454545454545464,\n                          'neg': 0.28181818181818186}),\n             'appa': defaultdict(int,\n                         {'neu': 0.42857142857142855,\n                          'pos': 0.14285714285714285,\n                          'neg': 0.42857142857142855}),\n             'roku': defaultdict(int,\n                         {'pos': 0.3666666666666667,\n                          'neu': 0.3777777777777778,\n                          'neg': 0.2555555555555554}),\n             'jet': defaultdict(int,\n                         {'neu': 0.5148514851485152,\n                          'pos': 0.2524752475247526,\n                          'neg': 0.2326732673267328}),\n             'zhang leader': defaultdict(int,\n                         {'neu': 0.4130434782608694,\n                          'pos': 0.21739130434782605,\n                          'neg': 0.3695652173913042}),\n             'ozai': defaultdict(int,\n                         {'pos': 0.2956521739130436,\n                          'neg': 0.2434782608695653,\n                          'neu': 0.46086956521739153}),\n             'hakoda': defaultdict(int,\n                         {'neu': 0.4927536231884051,\n                          'neg': 0.22463768115942037,\n                          'pos': 0.2826086956521739}),\n             'bato': defaultdict(int,\n                         {'neu': 0.5227272727272725,\n                          'pos': 0.284090909090909,\n                          'neg': 0.19318181818181815}),\n             'jeong jeong': defaultdict(int,\n                         {'neu': 0.5121951219512197,\n                          'neg': 0.28048780487804886,\n                          'pos': 0.20731707317073175}),\n             'the mechanist': defaultdict(int,\n                         {'neu': 0.5876288659793815,\n                          'neg': 0.24742268041237106,\n                          'pos': 0.16494845360824742}),\n             'yue': defaultdict(int,\n                         {'pos': 0.3043478260869564,\n                          'neu': 0.5434782608695651,\n                          'neg': 0.15217391304347822}),\n             'pakku': defaultdict(int,\n                         {'pos': 0.5131578947368417,\n                          'neu': 0.32894736842105243,\n                          'neg': 0.15789473684210525}),\n             'azula': defaultdict(int,\n                         {'neg': 0.22515212981744487,\n                          'neu': 0.4726166328600429,\n                          'pos': 0.3022312373225164}),\n             'chong': defaultdict(int,\n                         {'neg': 0.17910447761194026,\n                          'neu': 0.5373134328358207,\n                          'pos': 0.2835820895522387}),\n             'mai': defaultdict(int,\n                         {'neg': 0.2769230769230768,\n                          'neu': 0.46923076923076845,\n                          'pos': 0.25384615384615383}),\n             'ty lee': defaultdict(int,\n                         {'neu': 0.39393939393939414,\n                          'pos': 0.49494949494949525,\n                          'neg': 0.1111111111111111}),\n             'toph': defaultdict(int,\n                         {'neg': 0.18171021377672242,\n                          'neu': 0.5273159144893126,\n                          'pos': 0.29097387173396744}),\n             'professor zei': defaultdict(int,\n                         {'pos': 0.3278688524590164,\n                          'neu': 0.6065573770491807,\n                          'neg': 0.06557377049180328}),\n             'joo dee': defaultdict(int,\n                         {'neu': 0.5517241379310344,\n                          'pos': 0.2931034482758621,\n                          'neg': 0.15517241379310348}),\n             'long feng': defaultdict(int,\n                         {'pos': 0.2526315789473685,\n                          'neu': 0.42105263157894707,\n                          'neg': 0.3263157894736841}),\n             'earth king': defaultdict(int,\n                         {'neg': 0.25757575757575746,\n                          'neu': 0.3939393939393937,\n                          'pos': 0.3484848484848483}),\n             'guru pathik': defaultdict(int,\n                         {'neu': 0.41237113402061876,\n                          'pos': 0.3711340206185568,\n                          'neg': 0.2164948453608247}),\n             'piandao': defaultdict(int,\n                         {'neu': 0.5569620253164559,\n                          'pos': 0.30379746835443056,\n                          'neg': 0.13924050632911394}),\n             'hama': defaultdict(int,\n                         {'neg': 0.19191919191919196,\n                          'pos': 0.2727272727272728,\n                          'neu': 0.5353535353535355})})"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "get_characters_and_sentiment_frequencies(char_sentence_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('father', 95.04460042309827),\n ('uncle', 79.8199937280543),\n ('mai', 68.43211230463076),\n ('raiders', 58.964814717536434),\n ('stone', 49.13734559794703),\n ('changed', 45.62140820308717),\n ('dragon', 45.62140820308717),\n ('dragons', 45.62140820308717),\n ('history', 42.17979571865833),\n ('meeting', 42.17979571865833),\n ('firebending', 41.38814489602815),\n ('killed', 39.30987647835762),\n ('accepted', 39.30987647835762),\n ('sages', 39.30987647835762)]"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "season_tfidf[3]['zuko'][1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_tfidf['all']['suki'][1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38364bitavatarscriptanalysispipenv8de1ba2f463a4d24ae2f4934f1108141",
   "display_name": "Python 3.8.3 64-bit ('avatar-script-analysis': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}