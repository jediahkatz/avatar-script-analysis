{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_characters = [\n",
    "    'sokka', 'aang', 'katara', 'zuko', 'toph', 'iroh', 'zuko', 'azula', # 1000+ non-stopwords\n",
    "    'zhao', 'jet', 'suki', 'hakoda', 'bumi',                            # 400+ non-stopwords\n",
    "    'hama', 'ozai', 'guru pathik', 'mai', 'roku', 'long feng',          # 350+ non-stopwords\n",
    "    'the mechanist', 'piandao', 'warden', 'bato', 'ty lee',\n",
    "    # 'jeong jeong', 'pakku', 'zhang leader', 'earth king',\n",
    "    # 'professor zei', 'joo dee', 'chong', 'yue',                         # 200+ non-stopwords\n",
    "    # 'appa', 'momo'                                                      # :)\n",
    "]\n",
    "aliases = {\n",
    "    'fire lord ozai': 'ozai', 'ruko': 'roku', 'princess yue': 'yue', 'princess yue / moon spirit': 'yue', 'monk gyatso': 'gyatso',\n",
    "    'themechanist': 'the mechanist', 'zuko/blue spirit': 'zuko', 'king bumi': 'bumi', 'master pakku': 'pakku'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def get_ep_character_bows(script_lines):\n",
    "    \"\"\"Return a set of bags of words for this episode as a mapping from characters to words to counts.\"\"\"\n",
    "    bow = defaultdict(Counter)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        if speaker not in important_characters:\n",
    "            continue\n",
    "        line = line.lower()\n",
    "        # Get all non-stop words\n",
    "        stop_words = set(w for w in stopwords.words('english') \n",
    "            + ['us', 'get', 'like', 'thats', 'go', 'going', 'cant', 'yeh', 'oh', 'got', 'hey', \n",
    "               'yeah', 'uh', 'whats', 'could', 'shall', 'gonna', 'okay', 'one'] \n",
    "        )\n",
    "        words = re.findall(r'\\w+', line)\n",
    "        bow[speaker]['total_count'] += len(words)\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        bow[speaker].update(words)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_bows(bows):\n",
    "    \"\"\"Merge a collection of bags of words into one. Does not modify the input.\"\"\"\n",
    "    combined_bow = Counter()\n",
    "    for bow in bows:\n",
    "        combined_bow.update(bow)\n",
    "    return combined_bow\n",
    "\n",
    "def combine_character_bows(character_bows):\n",
    "    \"\"\"Merge a collection of mappings from characters to bags of words into one. Does not modify the input.\"\"\"\n",
    "    all_characters = set(k for bow in character_bows for k in bow.keys())\n",
    "    combined_bow_set = {c: combine_bows(bow_map[c] for bow_map in character_bows) for c in all_characters}\n",
    "    return defaultdict(Counter, combined_bow_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_by_bayes(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, p), where w is a word \n",
    "    and p is the probability that a random occurrence of the word was spoken by that character.\n",
    "    The lists are sorted by descending probability.\n",
    "    \"\"\"\n",
    "    # Compute the total number of occurrences for each word\n",
    "    combined_bow = combine_bows(character_bows.values())\n",
    "    # p(character | word) = [# times character says word] / [# occurrences of word]\n",
    "    words_by_bayes = {\n",
    "        c: sorted(\n",
    "            [(w, character_bows[c][w] / combined_bow[w]) for w in combined_bow.keys()],\n",
    "            key=lambda wp: wp[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "# Since we have very few characters, we want to weight idf highly (with a lower base)\n",
    "base = 1.3\n",
    "\n",
    "def get_words_by_tfidf(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, s), where w is a word \n",
    "    and s is the tf-idf score of that word for that character. The lists are sorted \n",
    "    by descending score.\n",
    "    \"\"\"\n",
    "    # Compute the number of characters that said each word\n",
    "    all_words = set(k for bow in character_bows.values() for k in bow.keys())\n",
    "    n_chars = len(character_bows.keys())\n",
    "    idf = {\n",
    "        w: 1 + log(n_chars / (1 + sum(character_bows[c][w] > 0 for c in character_bows.keys())), base)\n",
    "        for w in all_words\n",
    "    }\n",
    "    # tf-idf = tf(w, c) / (1 + log(N / (1 + df(w))))\n",
    "    words_by_tfidf = {\n",
    "        c: sorted(\n",
    "            [(w, (1 + v) * idf[w]) for w, v in character_bows[c].items()],\n",
    "            key=lambda ws: ws[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "season_bows_collection = defaultdict(list)\n",
    "season_bows = {}\n",
    "season_tfidf = {}\n",
    "for season in range(1, 4):\n",
    "    for episode in range(1, 21):\n",
    "        ep_num = f'{season}{str(episode).zfill(2)}'\n",
    "        with open(f'transcripts/{ep_num}.txt') as f:\n",
    "            season_bows_collection[season].append(get_ep_character_bows(f.readlines()))\n",
    "    season_bows[season] = combine_character_bows(season_bows_collection[season])\n",
    "    season_tfidf[season] = get_words_by_tfidf(season_bows[season])\n",
    "season_bows['all'] = combine_character_bows([bows for s in range(1, 4) for bows in season_bows_collection[s]])\n",
    "season_tfidf['all'] = get_words_by_tfidf(season_bows['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('uncle', 256.8371058658668),\n ('avatar', 55.51531380622062),\n ('ship', 47.54313991570178),\n ('zhao', 42.95382034794852),\n ('hiding', 35.67182025914817),\n ('trail', 35.67182025914817),\n ('helmsman', 34.719168628118204),\n ('need', 30.55015405801113),\n ('capture', 30.189326806092055),\n ('lose', 30.189326806092055),\n ('safety', 28.537456207318538),\n ('underestimated', 26.03937647108865),\n ('throne', 26.03937647108865),\n ('sealed', 26.03937647108865)]"
     },
     "metadata": {},
     "execution_count": 929
    }
   ],
   "source": [
    "season_tfidf[1]['zuko'][1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('unagi', 35.05425857283129),\n ('sokka', 34.920915662359135),\n ('appa', 32.03346819537735),\n ('warrior', 25.047772179468616),\n ('gondola', 23.001197857789794),\n ('throwing', 20.617985496815475),\n ('uniform', 20.617985496815475),\n ('threads', 20.617985496815475),\n ('symbolizes', 20.617985496815475),\n ('veins', 20.617985496815475),\n ('represents', 20.617985496815475),\n ('loosen', 20.617985496815475),\n ('crabby', 20.617985496815475),\n ('sleeveless', 20.617985496815475)]"
     },
     "metadata": {},
     "execution_count": 937
    }
   ],
   "source": [
    "season_tfidf['all']['suki'][1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('sokka', 19028),\n ('aang', 18858),\n ('katara', 15472),\n ('zuko', 9694),\n ('toph', 5677),\n ('iroh', 5437),\n ('azula', 3808),\n ('zhao', 1679),\n ('jet', 1444),\n ('suki', 1278),\n ('hakoda', 1105),\n ('bumi', 1045)]"
     },
     "metadata": {},
     "execution_count": 882
    }
   ],
   "source": [
    "sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38364bitavatarscriptanalysispipenv8de1ba2f463a4d24ae2f4934f1108141",
   "display_name": "Python 3.8.3 64-bit ('avatar-script-analysis': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}