{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_characters = {\n",
    "    'sokka', 'aang', 'katara', 'zuko', 'toph', 'iroh', 'zuko', 'azula', # 1000+ non-stopwords / 3000+ total words\n",
    "    'zhao', 'jet', 'suki', 'hakoda', 'bumi',                            # 400+ non-stopwords / 1000+ total words\n",
    "}\n",
    "minor_characters = {\n",
    "    'hama', 'ozai', 'guru pathik', 'mai', 'roku', 'long feng',          # 350+ non-stopwords\n",
    "    'the mechanist', 'piandao', 'warden', 'bato', 'ty lee',\n",
    "    'jeong jeong', 'pakku', 'earth king', # 'zhang leader'\n",
    "    'professor zei', 'joo dee', 'chong', 'yue',                         # 200+ non-stopwords\n",
    "    'appa', 'momo'                                                      # :)\n",
    "}\n",
    "aliases = {\n",
    "    'fire lord ozai': 'ozai', 'ruko': 'roku', 'princess yue': 'yue', 'princess yue / moon spirit': 'yue', 'monk gyatso': 'gyatso',\n",
    "    'themechanist': 'the mechanist', 'zuko/blue spirit': 'zuko', 'king bumi': 'bumi', 'master pakku': 'pakku'\n",
    "}\n",
    "word_substitutions = {\n",
    "    'jeong jeong': 'jeong_jeong', 'long feng': 'long_feng', 'ty lee': 'ty_lee', 'joo dee': 'joo_dee', 'wan shi tong': 'wan_shi_tong', \n",
    "    'chit sang': 'chit_sang', 'ba sing se': 'ba_sing_se', 'twinkle toes': 'twinkletoes', 'twinkle-toes': 'twinkletoes', 'mum': 'mom',\n",
    "    'dai li': 'dai_li', 'sozen': 'sozin'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "def get_ep_character_bows(script_lines):\n",
    "    \"\"\"Return a set of bags of words for this episode as a mapping from characters to words to counts.\"\"\"\n",
    "    bow = defaultdict(Counter)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        line = line.lower()\n",
    "        # Get all non-stop words\n",
    "        stop_words = set(w for w in stopwords.words('english') \n",
    "            + ['us', 'get', 'like', 'thats', 'go', 'going', 'cant', 'oh', 'got', 'hey', 'would',\n",
    "               'yeah', 'yeh', 'ya', 'uh', 'whats', 'could', 'shall', 'gonna', 'okay', 'one', \n",
    "               'something', 'may'] \n",
    "        )\n",
    "        # Special cases, typically multi-word names treated as one word\n",
    "        words = [w.replace('_', ' ') for w in\n",
    "            re.findall(r'\\w+', \n",
    "                reduce(lambda acc, k: acc.replace(k, word_substitutions[k]), word_substitutions.keys(), line)\n",
    "            )]\n",
    "        bow[speaker]['total_count'] += len(words)\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        bow[speaker].update(words)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ep_total_word_count(script_lines):\n",
    "    return sum(len(re.findall(r'\\w+', line.split('::')[1])) for line in script_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_bows(bows):\n",
    "    \"\"\"Merge a collection of bags of words into one. Does not modify the input.\"\"\"\n",
    "    combined_bow = Counter()\n",
    "    for bow in bows:\n",
    "        combined_bow.update(bow)\n",
    "    return combined_bow\n",
    "\n",
    "def combine_character_bows(character_bows):\n",
    "    \"\"\"Merge a collection of mappings from characters to bags of words into one. Does not modify the input.\"\"\"\n",
    "    all_characters = set(k for bow in character_bows for k in bow.keys())\n",
    "    combined_bow_set = {c: combine_bows(bow_map[c] for bow_map in character_bows) for c in all_characters}\n",
    "    return defaultdict(Counter, combined_bow_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is a bad metric -- it just finds words that this character said once and nobody else ever said.\n",
    "def get_words_by_bayes(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, p), where w is a word \n",
    "    and p is the probability that a random occurrence of the word was spoken by that character.\n",
    "    The lists are sorted by descending probability.\n",
    "    \"\"\"\n",
    "    # Compute the total number of occurrences for each word\n",
    "    combined_bow = combine_bows(character_bows.values())\n",
    "    # p(character | word) = [# times character says word] / [# occurrences of word]\n",
    "    words_by_bayes = {\n",
    "        c: sorted(\n",
    "            [(w, character_bows[c][w] / combined_bow[w]) for w in combined_bow.keys()],\n",
    "            key=lambda wp: wp[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "# Since we have very few characters, we want to weight idf highly (with a lower base)\n",
    "base = 1.2\n",
    "\n",
    "# I experimented with these parameters but ultimately decided they were more harmful than helpful on such a small dataset\n",
    "def get_words_by_tfidf(character_bows, character_set=None, sublinear_tf=False, filter_df_1=False):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, s), where w is a word \n",
    "    and s is the tf-idf score of that word for that character. The lists are sorted \n",
    "    by descending score.\n",
    "    \"\"\"\n",
    "    # If no explicit character set is passed, just use all characters\n",
    "    character_set = character_set or character_bows.keys()\n",
    "    # Compute the number of characters that said each word\n",
    "    all_words = set(k for bow in character_bows.values() for k in bow.keys())\n",
    "    n_chars = len(character_set)\n",
    "    idf = {\n",
    "        w: 1 + log(n_chars / (1 + df), base)\n",
    "        for w in all_words if (df := sum(character_bows[c][w] > 0 for c in character_set)) > (1 if filter_df_1 else 0)\n",
    "    }\n",
    "    # tf-idf = tf(w, c) / (1 + log(N / (1 + df(w))))\n",
    "    words_by_tfidf = {\n",
    "        c: sorted([\n",
    "                (w, ((1 + log(v)) if sublinear_tf else v) * idf[w]) \n",
    "                for w, v in character_bows[c].items() \n",
    "                if w != 'total_count' and (not filter_df_1 or w in idf)\n",
    "            ],\n",
    "            key=lambda ws: ws[1],\n",
    "            reverse=True\n",
    "        ) for c in character_set\n",
    "    }\n",
    "    return words_by_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_character_sentence_sentiments(script_lines, character_set=None):\n",
    "    \"\"\"\n",
    "    Return a mapping from each character to a list of all their sentences along with the\n",
    "    computed sentiment scores (positive, negative, neutral) for each sentence.\n",
    "    \"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    char_sentence_sentiments = defaultdict(list)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        if character_set and speaker not in character_set:\n",
    "            continue\n",
    "        \n",
    "        char_sentence_sentiments[speaker].extend(\n",
    "            (sid.polarity_scores(sentence), sentence) for sentence in tokenizer.tokenize(line)\n",
    "        )\n",
    "\n",
    "    return char_sentence_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_character_sentence_sentiments(target_map, added_map):\n",
    "    \"\"\"\n",
    "    Given two maps from characters to lists of sentence sentiments, combine all lists with the same key.\n",
    "    The first argument will be modified.\n",
    "    \"\"\"\n",
    "    for char in added_map.keys():\n",
    "        target_map[char].extend(added_map[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_sentiment_frequencies(char_sentence_sentiments):\n",
    "    \"\"\"\n",
    "    Given a map from characters to lists of sentence sentiments, return a map\n",
    "    from characters to lists of the frequency of positive, negative, and neutral\n",
    "    sentiments.\n",
    "    \"\"\"\n",
    "    sentiment_freqs = defaultdict(lambda: defaultdict(int))\n",
    "    for char, sentiments in char_sentence_sentiments.items():\n",
    "        for (scores, _) in sentiments:\n",
    "            sentiment_freqs[char][\n",
    "                'pos' if scores['compound'] > 0.05 else 'neg' if scores['compound'] < -0.05 else 'neu'\n",
    "            ] += 1/len(sentiments)\n",
    "    return sentiment_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "season_bows_collection = defaultdict(list)\n",
    "season_bows = {}\n",
    "season_tfidf = {}\n",
    "season_tfidf_minor = {}\n",
    "season_bayes = {}\n",
    "total_word_count = {}\n",
    "character_sentence_sentiments = defaultdict(list)\n",
    "for season in range(1, 4):\n",
    "    for episode in range(1, 21):\n",
    "        ep_num = f'{season}{str(episode).zfill(2)}'\n",
    "        with open(f'transcripts/{ep_num}.txt') as f:\n",
    "            script_lines = f.readlines()\n",
    "            season_bows_collection[season].append(get_ep_character_bows(script_lines))\n",
    "            merge_character_sentence_sentiments(character_sentence_sentiments, get_character_sentence_sentiments(script_lines))\n",
    "            total_word_count[ep_num] = get_ep_total_word_count(script_lines)\n",
    "    season_bows[season] = combine_character_bows(season_bows_collection[season])\n",
    "    # It helps to compute tf-idf scores for major characters only (excluding minor characters) or\n",
    "    # else depth-of-character words like \"know\", \"want\", \"think\" are weighted too highly\n",
    "    season_tfidf[season] = get_words_by_tfidf(season_bows[season], character_set=major_characters)\n",
    "    season_tfidf_minor[season] = get_words_by_tfidf(season_bows[season], character_set=major_characters | minor_characters)\n",
    "    season_bayes[season] = get_words_by_bayes(season_bows[season])\n",
    "season_bows['all'] = combine_character_bows([bows for s in range(1, 4) for bows in season_bows_collection[s]])\n",
    "season_tfidf['all'] = get_words_by_tfidf(season_bows['all'], character_set=major_characters)\n",
    "season_tfidf_minor['all'] = get_words_by_tfidf(season_bows['all'], character_set=major_characters | minor_characters)\n",
    "season_bayes['all'] = get_words_by_bayes(season_bows['all'])\n",
    "character_sentiment_frequencies = get_character_sentiment_frequencies(character_sentence_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dump\n",
    "\n",
    "results_json = defaultdict(dict)\n",
    "for c in sorted(major_characters | minor_characters):\n",
    "    ct = c.title()\n",
    "    for season in ['all', 1, 2, 3]:\n",
    "        top_ten = [\n",
    "            w for (w, _) in (\n",
    "                season_tfidf[season][c][:10] if c in major_characters else season_tfidf_minor[season][c][:10]\n",
    "            )\n",
    "        ]\n",
    "        results_json[ct][season] = top_ten\n",
    "    results_json[ct]['word_count'] = season_bows['all'][c]['total_count']\n",
    "    results_json[ct]['neg_freq'] = character_sentiment_frequencies[c]['neg']\n",
    "    results_json[ct]['neu_freq'] = character_sentiment_frequencies[c]['neu']\n",
    "    results_json[ct]['pos_freq'] = character_sentiment_frequencies[c]['pos']\n",
    "    results_json[ct]['net_freq'] = character_sentiment_frequencies[c]['pos'] - character_sentiment_frequencies[c]['neg']\n",
    "\n",
    "with open('results.json', 'w') as f:\n",
    "    dump(results_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('sokka', 18998),\n ('aang', 18830),\n ('katara', 15449),\n ('zuko', 9681),\n ('toph', 5661),\n ('iroh', 5427),\n ('azula', 3783),\n ('zhao', 1678),\n ('jet', 1440),\n ('suki', 1278),\n ('hakoda', 1103),\n ('bumi', 1043),\n ('ozai', 946),\n ('hama', 936),\n ('guru pathik', 862),\n ('mai', 844),\n ('roku', 806),\n ('long feng', 761),\n ('warden', 746),\n ('bato', 696),\n ('piandao', 689),\n ('the mechanist', 682),\n ('ty lee', 675),\n ('pakku', 587),\n ('yue', 578),\n ('jeong jeong', 562),\n ('chong', 533),\n ('earth king', 512),\n ('joo dee', 477),\n ('zhang leader', 469)]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Sort by total word count\n",
    "sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('sokka', 7666),\n ('aang', 7337),\n ('katara', 5985),\n ('zuko', 3587),\n ('iroh', 2317),\n ('toph', 2164),\n ('azula', 1517),\n ('zhao', 729),\n ('jet', 562),\n ('suki', 480),\n ('hakoda', 464),\n ('bumi', 458),\n ('hama', 383),\n ('guru pathik', 380),\n ('ozai', 380),\n ('roku', 349),\n ('mai', 345),\n ('long feng', 324),\n ('the mechanist', 322),\n ('piandao', 303),\n ('warden', 294),\n ('bato', 275),\n ('ty lee', 267),\n ('pakku', 262),\n ('jeong jeong', 247),\n ('zhang leader', 231),\n ('earth king', 217),\n ('chong', 214),\n ('professor zei', 207),\n ('yue', 203)]"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := sum(season_bows['all'][char].values()) - season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38364bitavatarscriptanalysispipenv8de1ba2f463a4d24ae2f4934f1108141",
   "display_name": "Python 3.8.3 64-bit ('avatar-script-analysis': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}